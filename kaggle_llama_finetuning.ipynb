{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1 1B Fine-Tuning for Cybersecurity\n",
    "\n",
    "This notebook fine-tunes the `meta-llama/Llama-3.1-1B` model on a cybersecurity-related dataset. The dataset is expected to be in Arrow format and contain `problem_statement` and `output` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers langchain langchain-community pyarrow PyYAML requests beautifulsoup4 huggingface_hub datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# It is recommended to set your Hugging Face token as a secret in Kaggle\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "# login(token=hf_token)\n",
    "\n",
    "# For simplicity in this example, we will paste it directly. DO NOT DO THIS IN A PUBLIC NOTEBOOK\n",
    "hf_token = \"your_hugging_face_token_here\" # Replace with your token\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model': {\n",
    "        'name': 'meta-llama/Llama-3.1-1B',\n",
    "        'fine_tuned_path': 'models/llama3-1B-cybersec-finetuned'\n",
    "    },\n",
    "    'dataset_path': '/kaggle/input/your-dataset-name/problem_solution_data.arrow', # <-- IMPORTANT: Change this to your dataset path in Kaggle\n",
    "    'hyperparameters': {\n",
    "        'learning_rate': 2e-5,\n",
    "        'batch_size': 4,\n",
    "        'epochs': 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "def fine_tune_model(config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model']['name'])\n",
    "    # Set the padding token if it's not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(config['model']['name'])\n",
    "\n",
    "    with pa.OSFile(config['dataset_path'], 'rb') as source:\n",
    "        table = pa.ipc.open_file(source).read_all()\n",
    "    df = table.to_pandas()\n",
    "    texts = [f"Problem: {row['problem_statement']}\nSolution: {row['output']}" for _, row in df.iterrows()]\n",
    "    dataset = Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "        \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "    # Add the 'labels' column, which is required for training\n",
    "    tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config['model']['fine_tuned_path'],\n",
    "        num_train_epochs=config['hyperparameters']['epochs'],\n",
    "        per_device_train_batch_size=config['hyperparameters']['batch_size'],\n",
    "        learning_rate=config['hyperparameters']['learning_rate'],\n",
    "        # Add evaluation strategy to monitor performance during training\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        # Add save strategy to save the model checkpoints\n",
    "        save_strategy=\"epoch\",\n",
    "        # Load the best model at the end of training\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        eval_dataset=tokenized_dataset, # Using the same dataset for evaluation as an example\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(config['model']['fine_tuned_path'])\n",
    "    tokenizer.save_pretrained(config['model']['fine_tuned_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # You can push your fine-tuned model to the Hugging Face Hub\n",
    " # from huggingface_hub import HfApi\n",
    " # api = HfApi()\n",
    " # \n",
    " # api.upload_folder(\n",
    " #     folder_path=config['model']['fine_tuned_path'],\n",
    " #     repo_id=\"your-username/your-model-name\", #<-- Change this to your repo id\n",
    " #     repo_type=\"model\",\n",
    " # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
